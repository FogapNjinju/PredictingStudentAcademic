{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9b2bcfd",
   "metadata": {},
   "source": [
    "#### Purpose : D. MODEL DEVELOPMENT: HANDLING OUTLIERS AND MISSING VALUES, DATA NORMALIZATION, FEATURE SELECTION, HYPERPARAMETER TUNING VIA GRID SEARCH, AND PREVENTION OF DATA LEAKAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5133241",
   "metadata": {},
   "source": [
    "In this phase, the focus is on preparing the dataset and building predictive models to accurately forecast student academic performance. Key preprocessing steps include handling outliers and missing values, normalizing data where necessary, and performing feature selection to retain the most informative features while reducing redundancy. To optimize model performance, hyperparameter tuning is conducted using GridSearchCV with 5-fold cross-validation. The models selected for training include Support Vector Machines (SVM), Logistic Regression, Random Forest, XGBoost, and Gradient Boosting, providing a mix of linear, non-linear, and ensemble approaches to capture complex patterns in the data. Care is taken throughout to prevent data leakage, ensuring that all transformations and tuning steps are applied only on training data within the cross-validation framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66927a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 00_project_setup.ipynb\n",
    "%run 01_data_import.ipynb \n",
    "%run 04_feature_engineering.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aad2d56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = reduced_df.drop('Target', axis=1)\n",
    "y = reduced_df['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e7e5929",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = LabelEncoder()\n",
    "y_copy = y.values.ravel()\n",
    "y_encoded = le.fit_transform(y_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af3e4e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset (optional: holdout for final testing)\n",
    "# -----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96aed945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Define models and hyperparameters\n",
    "# -----------------------------\n",
    "models = {\n",
    "    \"SVM\": {\n",
    "        \"model\": SVC(probability=True, random_state=42),\n",
    "        \"params\": {\n",
    "            \"clf__C\": [0.1, 1, 10],\n",
    "            \"clf__kernel\": [\"linear\", \"rbf\"],\n",
    "            \"clf__gamma\": [\"scale\", \"auto\"]\n",
    "        }\n",
    "    },\n",
    "    \"LogisticRegression\": {\n",
    "        \"model\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "        \"params\": {\n",
    "            \"clf__C\": [0.1, 1, 10],\n",
    "            \"clf__penalty\": [\"l2\"],\n",
    "            \"clf__solver\": [\"lbfgs\"]\n",
    "        }\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"model\": RandomForestClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            \"clf__n_estimators\": [100, 200],\n",
    "            \"clf__max_depth\": [None, 5, 10],\n",
    "            \"clf__min_samples_split\": [2, 5]\n",
    "        }\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"model\": XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\", random_state=42),\n",
    "        \"params\": {\n",
    "            \"clf__n_estimators\": [100, 200],\n",
    "            \"clf__max_depth\": [3, 5],\n",
    "            \"clf__learning_rate\": [0.01, 0.1]\n",
    "        }\n",
    "    },\n",
    "    \"GradientBoosting\": {\n",
    "        \"model\": GradientBoostingClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            \"clf__n_estimators\": [100, 200],\n",
    "            \"clf__learning_rate\": [0.05, 0.1],\n",
    "            \"clf__max_depth\": [3, 5]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76abf244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Define preprocessing pipeline\n",
    "# -----------------------------\n",
    "def create_pipeline(model):\n",
    "    pipeline = Pipeline([\n",
    "        # 1. Handle missing values\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        # 2. Feature scaling\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        # 3. Classifier\n",
    "        (\"clf\", model)\n",
    "    ])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77cb0a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training SVM...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best parameters for SVM: {'clf__C': 10, 'clf__gamma': 'scale', 'clf__kernel': 'linear'}\n",
      "Best cross-validation F1 score for SVM: 0.7527\n",
      "\n",
      "Training LogisticRegression...\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best parameters for LogisticRegression: {'clf__C': 1, 'clf__penalty': 'l2', 'clf__solver': 'lbfgs'}\n",
      "Best cross-validation F1 score for LogisticRegression: 0.7454\n",
      "\n",
      "Training RandomForest...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best parameters for RandomForest: {'clf__max_depth': 10, 'clf__min_samples_split': 2, 'clf__n_estimators': 100}\n",
      "Best cross-validation F1 score for RandomForest: 0.7553\n",
      "\n",
      "Training XGBoost...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best parameters for XGBoost: {'clf__learning_rate': 0.1, 'clf__max_depth': 5, 'clf__n_estimators': 200}\n",
      "Best cross-validation F1 score for XGBoost: 0.7592\n",
      "\n",
      "Training GradientBoosting...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best parameters for GradientBoosting: {'clf__learning_rate': 0.1, 'clf__max_depth': 5, 'clf__n_estimators': 100}\n",
      "Best cross-validation F1 score for GradientBoosting: 0.7604\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Model Building: GridSearchCV with 5-fold CV\n",
    "# -----------------------------\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "best_models = {}\n",
    "\n",
    "for name, m in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    pipeline = create_pipeline(m[\"model\"])\n",
    "    \n",
    "    grid = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=m[\"params\"],\n",
    "        cv=skf,\n",
    "        scoring=\"f1_weighted\",\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid.fit(X_train, y_train)\n",
    "    best_models[name] = grid.best_estimator_\n",
    "    print(f\"Best parameters for {name}: {grid.best_params_}\")\n",
    "    print(f\"Best cross-validation F1 score for {name}: {grid.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5326b70f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
