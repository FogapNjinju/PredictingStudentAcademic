{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9b2bcfd",
   "metadata": {},
   "source": [
    "#### Purpose : D. MODEL DEVELOPMENT: HANDLING OUTLIERS AND MISSING VALUES, DATA NORMALIZATION, FEATURE SELECTION, HYPERPARAMETER TUNING VIA GRID SEARCH, AND PREVENTION OF DATA LEAKAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5133241",
   "metadata": {},
   "source": [
    "In this phase, the focus is on preparing the dataset and building predictive models to accurately forecast student academic performance. Key preprocessing steps include handling outliers and missing values, normalizing data where necessary, and performing feature selection to retain the most informative features while reducing redundancy. To optimize model performance, hyperparameter tuning is conducted using GridSearchCV with 5-fold cross-validation. The models selected for training include Support Vector Machines (SVM), Logistic Regression, Random Forest, XGBoost, and Gradient Boosting, providing a mix of linear, non-linear, and ensemble approaches to capture complex patterns in the data. Care is taken throughout to prevent data leakage, ensuring that all transformations and tuning steps are applied only on training data within the cross-validation framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66927a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (0.14.0)\n",
      "Requirement already satisfied: numpy<3,>=1.25.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy<2,>=1.11.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.4.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.7.2)\n",
      "Requirement already satisfied: joblib<2,>=1.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (3.5.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (0.14.0)\n",
      "Requirement already satisfied: numpy<3,>=1.25.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy<2,>=1.11.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.4.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.7.2)\n",
      "Requirement already satisfied: joblib<2,>=1.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (3.5.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (0.14.0)\n",
      "Requirement already satisfied: numpy<3,>=1.25.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy<2,>=1.11.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.4.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.7.2)\n",
      "Requirement already satisfied: joblib<2,>=1.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (3.5.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (0.14.0)\n",
      "Requirement already satisfied: numpy<3,>=1.25.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy<2,>=1.11.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.4.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.7.2)\n",
      "Requirement already satisfied: joblib<2,>=1.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "%run 00_project_setup.ipynb\n",
    "%run 01_data_import.ipynb \n",
    "%run 04_feature_engineering.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aad2d56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = reduced_df.drop('Target', axis=1)\n",
    "y = reduced_df['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e7e5929",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = LabelEncoder()\n",
    "y_copy = y.values.ravel()\n",
    "y_encoded = le.fit_transform(y_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8b73c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: (4424, 15)\n",
      "After outlier removal: (4335, 15)\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# 1. OUTLIER REMOVAL BEFORE TRAIN/TEST SPLIT\n",
    "# --------------------------------------------------\n",
    "outlier_detector = IsolationForest(contamination=0.02, random_state=42)\n",
    "outlier_detector.fit(X)\n",
    "\n",
    "inliers = outlier_detector.predict(X) == 1\n",
    "X_clean = X[inliers]\n",
    "y_clean = y_encoded[inliers]   # Use encoded y\n",
    "\n",
    "print(\"Original:\", X.shape)\n",
    "print(\"After outlier removal:\", X_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "750b7e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# 2. Train-test split\n",
    "# --------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_clean, y_clean, test_size=0.2, random_state=42, stratify=y_clean\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ee887c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------------------------\n",
    "# 3. Define models and hyperparameters\n",
    "# --------------------------------------------------\n",
    "models = {\n",
    "    \"SVM\": {\n",
    "        \"model\": SVC(probability=True, random_state=42),\n",
    "        \"params\": {\n",
    "            \"clf__C\": [0.1, 1, 10],\n",
    "            \"clf__kernel\": [\"linear\", \"rbf\"],\n",
    "            \"clf__gamma\": [\"scale\", \"auto\"]\n",
    "        }\n",
    "    },\n",
    "    \"LogisticRegression\": {\n",
    "        \"model\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "        \"params\": {\n",
    "            \"clf__C\": [0.1, 1, 10],\n",
    "            \"clf__penalty\": [\"l2\"],\n",
    "            \"clf__solver\": [\"lbfgs\"]\n",
    "        }\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"model\": RandomForestClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            \"clf__n_estimators\": [100, 200],\n",
    "            \"clf__max_depth\": [None, 5, 10],\n",
    "            \"clf__min_samples_split\": [2, 5]\n",
    "        }\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"model\": XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\", random_state=42),\n",
    "        \"params\": {\n",
    "            \"clf__n_estimators\": [100, 200],\n",
    "            \"clf__max_depth\": [3, 5],\n",
    "            \"clf__learning_rate\": [0.01, 0.1]\n",
    "        }\n",
    "    },\n",
    "    \"GradientBoosting\": {\n",
    "        \"model\": GradientBoostingClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            \"clf__n_estimators\": [100, 200],\n",
    "            \"clf__learning_rate\": [0.05, 0.1],\n",
    "            \"clf__max_depth\": [3, 5]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53a09ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# 4. Preprocessing pipeline (without SMOTE)\n",
    "# --------------------------------------------------\n",
    "def create_pipeline(model):\n",
    "    return Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d196b266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# 5. Custom CV Strategy: SMOTE INSIDE CV LOOP\n",
    "# --------------------------------------------------\n",
    "class SMOTE_CV(GridSearchCV):\n",
    "    def _fit_and_score(self, estimator, X, y, scorer, train, test, **fit_params):\n",
    "        \n",
    "        # Apply SMOTE ONLY to training fold\n",
    "        sm = SMOTE(random_state=42)\n",
    "        X_train_res, y_train_res = sm.fit_resample(X[train], y[train])\n",
    "        \n",
    "        # Fit model\n",
    "        estimator.fit(X_train_res, y_train_res)\n",
    "        \n",
    "        # Predict on unmodified test fold\n",
    "        y_pred = estimator.predict(X[test])\n",
    "        \n",
    "        # Score\n",
    "        score = scorer(estimator, X[test], y[test])\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be8d185b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training SVM...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best parameters for SVM: {'clf__C': 10, 'clf__gamma': 'scale', 'clf__kernel': 'linear'}\n",
      "Best CV Weighted F1 Score: 0.7490\n",
      "\n",
      "Training LogisticRegression...\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best parameters for LogisticRegression: {'clf__C': 10, 'clf__penalty': 'l2', 'clf__solver': 'lbfgs'}\n",
      "Best CV Weighted F1 Score: 0.7455\n",
      "\n",
      "Training RandomForest...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best parameters for RandomForest: {'clf__max_depth': 10, 'clf__min_samples_split': 5, 'clf__n_estimators': 200}\n",
      "Best CV Weighted F1 Score: 0.7507\n",
      "\n",
      "Training XGBoost...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best parameters for XGBoost: {'clf__learning_rate': 0.1, 'clf__max_depth': 3, 'clf__n_estimators': 100}\n",
      "Best CV Weighted F1 Score: 0.7575\n",
      "\n",
      "Training GradientBoosting...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best parameters for GradientBoosting: {'clf__learning_rate': 0.1, 'clf__max_depth': 3, 'clf__n_estimators': 200}\n",
      "Best CV Weighted F1 Score: 0.7557\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# 6. GridSearch with SMOTE-enabled CV\n",
    "# --------------------------------------------------\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "best_models = {}\n",
    "\n",
    "for name, m in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "\n",
    "    pipeline = create_pipeline(m[\"model\"])\n",
    "\n",
    "    grid = SMOTE_CV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=m[\"params\"],\n",
    "        cv=skf,\n",
    "        scoring=\"f1_weighted\",\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Fit using SMOTE in CV\n",
    "    grid.fit(X_train , y_train)\n",
    "\n",
    "    best_models[name] = grid.best_estimator_\n",
    "\n",
    "    print(f\"Best parameters for {name}: {grid.best_params_}\")\n",
    "    print(f\"Best CV Weighted F1 Score: {grid.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4464a2b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
